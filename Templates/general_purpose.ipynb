{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard procedures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some boilerplate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = ''\n",
    "df = pd.read_csv(dataset_file, names=['cols'], index_col=0)\n",
    "\n",
    "print(\"The dataset size is {}\".format(df.shape))\n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['cols'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B: You should drop columns with a very similar distribution to other coloumns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting nulls (na) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} columns with missing values\".format(df.isna().sum().sum()))\n",
    "df = df.dropna()\n",
    "print(\"There are {} columns with missing values\".format(df.isna().sum().sum()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal Enconfing, for when we need to preserve the distance between values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder(dtype=int)\n",
    "column_to_transform = 'col_name' \n",
    "df[column_to_transform] = oe.fit_transform(df[column_to_transform].values.reshape(-1,1))\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoding (it is preferred when dealing with non-ordinal attributes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinMax scaling, especially useful in clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_processed = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplots (collection of scatterplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='col1', y='col2', data=df, hue=y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General stats info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General classifiers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "model_lbls = [\n",
    "              'dt', \n",
    "              'nb', \n",
    "              'lp', \n",
    "              'svc', \n",
    "             'knn',\n",
    "             'adb',\n",
    "             'rf',\n",
    "            ]\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_param_dt = [{'max_depth': [*range(1,20)]}]\n",
    "tuned_param_nb = [{'var_smoothing': [10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-07, 1e-8, 1e-9, 1e-10]}]\n",
    "tuned_param_lp = [{'early_stopping': [True]}]\n",
    "tuned_param_svc = [{'kernel': ['rbf'], \n",
    "                    'gamma': [1e-3, 1e-4],\n",
    "                    'C': [1, 10, 100, 1000],\n",
    "                    },\n",
    "                    {'kernel': ['linear'],\n",
    "                     'C': [1, 10, 100, 1000],                     \n",
    "                    },\n",
    "                   ]\n",
    "tuned_param_knn =[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "tuned_param_adb = [{'n_estimators':[20,30,40,50],\n",
    "                   'learning_rate':[0.5,0.75,1,1.25,1.5]}]\n",
    "tuned_param_rf = [{'max_depth': [*range(5,15)],\n",
    "                   'n_estimators':[*range(10,100,10)]}]\n",
    "\n",
    "models = {\n",
    "    'dt': {'name': 'Decision Tree       ',\n",
    "           'estimator': DecisionTreeClassifier(), \n",
    "           'param': tuned_param_dt,\n",
    "          },\n",
    "    'nb': {'name': 'Gaussian Naive Bayes',\n",
    "           'estimator': GaussianNB(),\n",
    "           'param': tuned_param_nb\n",
    "          },\n",
    "    'lp': {'name': 'Linear Perceptron   ',\n",
    "           'estimator': Perceptron(),\n",
    "           'param': tuned_param_lp,\n",
    "          },\n",
    "    'svc':{'name': 'Support Vector      ',\n",
    "           'estimator': SVC(), \n",
    "           'param': tuned_param_svc\n",
    "          },\n",
    "    'knn':{'name': 'K Nearest Neighbor ',\n",
    "           'estimator': KNeighborsClassifier(),\n",
    "           'param': tuned_param_knn\n",
    "       },\n",
    "       'adb':{'name': 'AdaBoost           ',\n",
    "           'estimator': AdaBoostClassifier(),\n",
    "           'param': tuned_param_adb\n",
    "          },\n",
    "    'rf': {'name': 'Random forest       ',\n",
    "           'estimator': RandomForestClassifier(),\n",
    "           'param': tuned_param_rf\n",
    "          }\n",
    "\n",
    "}\n",
    "\n",
    "scores = ['precision', 'recall']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning using cv (on previous defined estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "results_short = {}\n",
    "\n",
    "for score in scores:\n",
    "    print('='*40)\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    #'%s_macro' % score ## is a string formatting expression\n",
    "    # the parameter after % is substituted in the string placeholder %s\n",
    "    for m in model_lbls:\n",
    "        print('-'*40)\n",
    "        print(\"Trying model {}\".format(models[m]['name']))\n",
    "        \n",
    "        clf = GridSearchCV(models[m]['estimator'], models[m]['param'], cv=5,\n",
    "                           scoring='%s_macro' % score, \n",
    "                           return_train_score = False,\n",
    "                           n_jobs = 2, # this allows using multi-cores\n",
    "                           )\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        print_results(clf)\n",
    "        results_short[m] = clf.best_score_\n",
    "    print(\"Summary of results for {}\".format(score))\n",
    "    print(\"Estimator\")\n",
    "    for m in results_short.keys():\n",
    "        print(\"{}\\t - score: {:4.2}%\".format(models[m]['name'], results_short[m]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    # if best is linear there is no gamma parameter\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    params = model.cv_results_['params']\n",
    "    for mean, std, params_tuple in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params_tuple))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k_range = list(range(2,11)) # set the range of k values to test \n",
    "\n",
    "parameters = [{'n_clusters': k_range\n",
    "                    , 'linkage' : ['ward', 'complete', 'average', 'single']}]\n",
    "pg = list(ParameterGrid(parameters))\n",
    "result_ac = []\n",
    "\n",
    "for i in range(len(pg)):\n",
    "    ac = AgglomerativeClustering(**(pg[i]))\n",
    "    y_ac = ac.fit_predict(df)\n",
    "    result_ac.append([pg[i]['linkage'],pg[i]['n_clusters'],silhouette_score(df, y_ac)])\n",
    "\n",
    "\n",
    "# Dataframe with the results\n",
    "df_result_ac = pd.DataFrame(data = result_ac, columns=['linkage','n_clusters','silhouette_score'])\n",
    "df_result_ac.sort_values(by='silhouette_score', ascending=False).head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "min_points = 2*X.shape[1]\n",
    "\n",
    "param_grid = {'eps': list(np.arange(0.01, 1, 0.01)), 'min_samples': list(range(min_points-3,min_points+3,1))}\n",
    "params = list(ParameterGrid(param_grid))\n",
    "\n",
    "dbscan_out = pd.DataFrame(columns = ['eps','min_samples','silhouette', 'unclust%'])\n",
    "\n",
    "for i in range(len(params)) : \n",
    "    db = DBSCAN(**(params[i]))\n",
    "    \n",
    "    y_db = db.fit_predict(X)\n",
    "    cluster_labels_all = np.unique(y_db)\n",
    "    n_clusters = len(cluster_labels_all[cluster_labels_all != -1])\n",
    "\n",
    "    if n_clusters > 1:\n",
    "        X_cl = X[y_db!=-1,:]\n",
    "        y_db_cl = y_db[y_db!=-1]\n",
    "        silhouette = silhouette_score(X_cl,y_db_cl)\n",
    "        uncl_p = (1 - y_db_cl.shape[0]/y_db.shape[0]) * 100\n",
    "        dbscan_out.loc[len(dbscan_out)] = [db.eps, db.min_samples, n_clusters, silhouette, uncl_p]\n",
    "\n",
    "dbscan_out.sort_values(by=['silhouette'], ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster centers (centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = cluster_labels_all[cluster_labels_all != -1]\n",
    "\n",
    "cluster_centers = np.empty((n_clusters, X.shape[1]))\n",
    "for i in cluster_labels:\n",
    "    cluster_centers[i,:] = np.mean(X[y_db==i,:], axis = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k_range = list(range(2,11)) # set the range of k values to test \n",
    "results = []\n",
    "\n",
    "for k_ in k_range : \n",
    "    estimator = KMeans(n_clusters=k_)\n",
    "    y_pred = estimator.fit_predict(df)\n",
    "    results.append([k_, silhouette_score(df, y_pred), estimator.inertia_])\n",
    "\n",
    "results = pd.DataFrame(data=results, columns=['n_clusters', 'sil_score', 'inertia'])\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot inertias in KMeans, useful for elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(k_range, results['inertia'], color='red')\n",
    "ax.set_xlabel('Number of clusters')\n",
    "ax.set_ylabel('Inertias', color='red')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(k_range, results['sil_score'], color='blue')\n",
    "ax2.set_ylabel('Silhouette scores', color='blue')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association rules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First way to format the transaction list into an items table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.get_dummies(df, prefix='', prefix_sep='', dummy_na=False)\n",
    "df1 = df0.drop(columns=['Item(s)']) # We drop the items column, as we dont need that information. \n",
    "\n",
    "# IMPORTANT!!!!\n",
    "df1 = df1.groupby(level=0, axis=1).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = (df\n",
    "            .groupby(['InvoiceNo', 'Description'])['Quantity'].sum()\n",
    "            .unstack().reset_index()\n",
    "            .fillna(0)\n",
    "            .set_index('InvoiceNo')\n",
    ")\n",
    "basket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori computation, find the value of min_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "min_supports = np.arange(0.20, 0.01, step=-0.01)\n",
    "selected_sup = 0\n",
    "\n",
    "for sup in min_supports : \n",
    "    frequent_itemsets = apriori(df, min_support=sup, use_colnames=True)\n",
    "\n",
    "    \n",
    "    itemsets_above_threshold = sum([len(itemset) >= 2 for itemset in frequent_itemsets.itemsets])\n",
    "    if itemsets_above_threshold >= 8 : \n",
    "        selected_sup = sup\n",
    "        break\n",
    "\n",
    "    print(\"min_support: {:0.4f} - number of itemsets with at least 2 items: {}\".format(\n",
    "        sup, itemsets_above_threshold\n",
    "    ))\n",
    "    \n",
    "print(\"Selected min_support value is: {:0.4f}\".format(selected_sup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rules = 10\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=selected_sup, use_colnames=True)\n",
    "min_confidence = np.arange(1, 0.1, step=-0.01)\n",
    "\n",
    "selected_min_confidence = 0\n",
    "for mt in min_confidence : \n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=mt)\n",
    "\n",
    "    print('Metric: \"confidence\" - min_metric: {:0.4f} - Number of rules: {}'.format(mt, len(rules)))\n",
    "\n",
    "    if len(rules) >= min_rules : \n",
    "        selected_min_confidence = mt\n",
    "        break;\n",
    "\n",
    "print(\"Selected confidence value is: {:0.4f}\".format(selected_min_confidence))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8 (default, Apr 13 2021, 19:58:26) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b18a9988e9255073277d135c7087261722b40fca21f654d051abd907f565cc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
