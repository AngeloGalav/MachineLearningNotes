[..]

The last leaf in the right of the tree is the consequence of a __training set error__. 
This means that either my model based on hte decision tree or the data are not able to discriminate completely the example. 

The model I generated with the available training data is not able to reproduce completely the ground truth. 

We have a limit in the type of the model (that is, the decision tree) or in the data. 

1.35% is the lower limit of the error that we make with the training data, which are the information we already know.
Since we need to use this model with the real data to produce something useful, a lower limit of the error is not limiting. 

For this reason, we will use the test set error (?).

## Overfitting
Definition: overfitting happens when the learning is affected by noise. 
When a learning algorithm is affected by noise, the performance on the test set is (much) worse than that on the training set.

A decision tree is a _hypothesis_ of the relationship between the predictor attributes and the class. Here are some definitions: 
- $h$ = hypothesis
- $error_{train}(h)$ = error of the hypothesis on the training set 
- $error_x(h)$ = error of the hypothesis on the entire dataset

What are the possible causes of overfitting?
1. Presence of noise 
	- individuals in the test set can have _bad values_ in the predicting attributes and/or in the class label.
2. Lack of representative instances 
	- Some situations of the real world can be underrepresented, or not represented at all, in the training set. 
	- This is quite common.

Generalization is using the model fitted for a small portion of data with a bigger portion of the data, different from the previous. 

A good hypothesis has a low generalization error, which means it works well on examples different from those in training. 

#### Occam's razor theory
"Everything should be made as simple as possible, but not simpler"

This means that simple theories are preferable to complex ones, and a long hypothesis that fits the data is more likely to be a coincidence. 
Pruning a decision tree is a way to simplify it, but we need precise guidelines for effective pruning. 

## General effect of model simplification

This is the effects of pruning summarized in a picture:
![[general_effect_model_simplification.png]]

What is in the middle is basically the best situation we could possibly have. 

The situation shown in the figure happens with ALL classification methods, not just decision trees. 
There's no best model, and we must try to find the optimal configuration. 

It means that our model must have like a slider that we must use to tune the behavior of the tree (?). 

The fitting process needs to make a choice between the 2 variables, and Scikit learn chooses randomly. 

This can be seen when using the code:
```python
random.state = 3
model = DecisionTreeClassificafier(random.state = random.state)
model.fit(x, y, max_depth = 2)
```
Depth, for a decision tree, basically defines the amount of pruning/simplification in the model. 

Let's suppose that we chose `random.state = 5`, and we instead have a training error of 3% instead of 5%. 
A good model should not give us surprises, so I should be able to forecast a worst case ever, like if an error shouldn't more than X%, so if I observe that changing only the random effects, the error changes from 3% to 5%, then I should not rely on 3%. 

What we could do is try some different random states randomly and consider the worst case. 


### Purity of a node
A node with high purity means that it has a low diversity of labels, so a node with only one class has high purity. 

Entropy is the measure of purity, and information gain is essentially the pursuit of purity. 

#### Gini Index
Consider $p$ node with $C_p$ classes. Which is the frequency of the wrong classification in class $j$ given by a random assignment based only on the class frequencies in the current node?

For class $j$:
- frequency $f_{p,j}$
- frequency of the other classes $1- f_{p,j}$
- probability of wrong assignment $f_{p,j} * (1 - f_{p,j})$

The Gini index is __the total probability of wrong classification__:
$$
\sum_j f_{p,j} * (1 - f_{p,j}) = \sum_j f_{p,j} - \sum_j f^2_{p,j}
= 1 - \sum_j f^2_{p,j} 
$$

The maximum value is when all the records are uniformly distributed over all the classes: 
$1 - 1/C_p$.
The minimum value is when all the records belong to the same class: 0.

The GINI Index is used by a lot of technologies. 
- When a node $p$ is split into $ds$ descendants, say $p_1, \dots, p_{ds}$. 
- Let $N_{p,i}$ and $N_p$ be the number of records in the $i$-th descendant node and in the root respectively.
- We choose the split giving the maximum reduction of the Gini Index:

### DT generation - DT induction

[..]

In principle, finding the best DT is NP-complete, since we are following a greedy approach.  Which means that the heuristic algorithms allow to find sub-optimal solution, but it is fast. 

Redundant attributes do not cause any difficulty, this is because st

[..]