## Frequent Itemset Generation
Given D items, there are $M = 2^D$ possible candidate itemsets.
The idea is to _reduce_ the _number of candidates_ $M$, by using pruning techniques. 
Or, we can reduce the number of comparisons $NM$.

#### Reducing Number of Candidates
__Apriori principle__: if an _itemset is frequent_, then _all of its subsets_ must also be _frequent_.
![[a_priori_principle.png]]
This means that the _support of an itemset_ never exceeds _the support of its subsets_. 
 
![[pruning_strategy.png]]
In this tree, the elements on top are contained in the elements at the bottom.
Based on the a-priori principle, if a subset of an item is not frequent, then its containing sets must also be infrequent. 

So, it is pretty much intuitive to see what we have to prune. 

## Apriori algorithm 

#### Pruning Candidate generation
Some definitions: 
- $C_k$ : candidate itemsets of size $k$.
- $L_k$ : frequent itemsets of size $k$.
	- we have $C_k$, and we are looking for $L_k$.
- $subset_k(c)$: set of the subsets of $c$ with $k$ elements.

![[graph_with_k.png]]

#### Join step
- Let $L_k$ be represented as a table with $k$ _columns_ where each _row_ is a _frequent itemset_.
- Let the items in each row of $L_k$ be in lexicographic order
	- This is because we want to generate a node. Generating a node, in this case, means making a query to the dataset. 
- $C_{k+1}$ is generated by a self join of $L_k$. In particular, this is the query that will generate the next candidates $C_{k+1}$. 

\[This is a complicated explanation for a stupid concept, you have been warned.\]

![[sql_query.png]]

Suppose that in the graph image that we've seen before, AB is infrequent (like is shown by default in the image). $L_2$ then will contain {AC, AD, AE, BC ...}. 
Now, from AC, we will need to generate ABC, ACD and ADE. How can we do this?
- Note: meaning, how can we generate ABC from AC and AD in a literal sense.

We will need to insert in $C_3$ the elements through a _self join_, with $L_2$ with itself, so that the first $k-1$ elements are all equal in the join, and then I add the elements at the end. 
- i.e. AB and AC -> A = A, B < C -> ABC (I don't want to generate ACB because it will be useless, the sets will be equal). 
	- \[I would have used a full join generates a table with _no repeating rows_, so it always generates a single set, without giving importance to the lexicographic order.  It would have been less complicated in my humble opinion\]

#### Prune step
Each $(k + 1)$-itemset _which includes_ a $k$-itemset which _is not in_ $L_k$ is _deleted_ from $C_{k+1}$:

- for all $c \in C_k$ do 
	- for all $s \in subset_{k-1}(c)$ do 
		- if $s \in L_{k-1}$ then 
			- delete $c$ from $C_k$ 
- return $C_k$

For example: Even though ABC has been deemed as infrequent, I could generate it again by another join (es. AC + BC) and thus it must be deleted from the new list of candidates.   

#### Frequent itemset generation algorithm 
- $L_1$ <- frequent 1–itemsets 
- $k$ <- 1 
- while $L_k = \emptyset$ do
	- $C_{k+1}$ = candidates generated from $L_k$ 
	- for all $t$ transaction in database do 
		- increment candidate count in $C_{k+1}$ for candidates found in $t$ 
	- $L_{k+1}$ <- ${c} \in C_{k+1}$ : $sup(c) \ge minsup$     _(we will keep all the surviving candidates with sup 
	- $k$ <- $k + 1$                                             _bigger than a threshold)_ 
- return $k$, $L_k$

Here's an example of the outcome of this algorithm: 
![[example_apriori.png]]

##### Factors Affecting Complexity of Apriori
- Choice of minimum support threshold 
- Dimensionality (number of items) of the data set
- Size of database
- Average transaction width

## Rule generation 

#### Confidence
The confidence of a rule can be computed from the supports:
$$
conf(A \implies C) = \dfrac{sup(A\implies C)}{sup(A)}
$$
#### Rule generation
Find all the non-empty subsets $f \in L$ such that the confidence of rule $f \implies (L - f)$ is not less than the minimum confidence (set by the experiment designer)
- ![[example_generation.png]]
- if $|L| = k$ then _there are $2^k - 2$ candidate rules_
	- $L \implies \emptyset$ and $\emptyset \implies L$ can be ignored

These are a lot of rules. How to efficiently generate rules from frequent itemsets?
- In general, confidence does not have an anti-monotone property 
	- $conf(ABC \rightarrow D)$ can be larger or smaller than $conf (AB \rightarrow D)$
	- But, let us consider rules generated from the same itemset:
	- ![[rule_confidence.png]]

![[rule_pruning.png]]
Notice: since we know that all the children rules of that itemset will have a lower confidence (and thus we are not interested in them), _we can prune_!
- In general, we prune a rule D=>ABC if _its subset_ AD=>BC _does not have high confidence_. 
- We can also use the _support_ $sup(.)$ for pruning.  

### Interestingness
Beside support and confidence, __Interestingness__ measures can be used to prune/rank the derived patterns.

#### Computing Interestingness Measures
Given a rule A => C, the information needed to _compute rule interestingness_ can be obtained from a _contingency table_.
![[contingency_table.png]]

Here's an example:
![[drawbacl_of_confidence.png]]
__We can't trust confidence__! 
- Note: this is a very specific case (coffee in itself has a _huge support_ in this database), in general confidence is useful. 

### Other interestingness measures (from Statistics)
#### Lift
![[lift.png]]

#### Leverage
![[leverage.png]]

#### Conviction
![[conviction.png]]

#### Intuition about measures
- higher support => rule applies to more records.
- higher confidence => chance that _the rule is true_ for _some record_ is higher.
- higher lift => chance that the rule is just a coincidence is lower. 
- higher conviction => the rule is violated less often than it would be if the antecedent and the consequent were independent.

- ==Confidence is usually the base tool==. 
- Other measures can be used to test the results given by confidence and for _additional filtering_. 

## Transforming a relational DB into a transaction DB
A.k.a. transforming from multi-dimensional to mono-dimensional. 

Let’s consider a dataset deriving from sensors measuring the concentration of air pollutants:
![[table_example_1.png]]

We could represent a transactional table from this DB by using, for each row, a tuple of booleans stating _if the value is present_ or not (or we can use values in the in domain of the attribute).   
Then:
- Look for rules such as _CO = high_ and _Tin Oxide = high_, then _Titanium = high_ (support 0.25 and confidence 1 in this dataset). 
	- This way I can forecast data if, for example, values for a given sensors are not present.


### Multi to mono 
- Mono–dimensional (intra-attribute) 
	- event: transaction 
	- event description: items A, B, and C are together in a transaction 
- Multi–dimensional (inter–attribute) 
	- event: tuple 
	- event description: attribute A has value a, attribute B has value b and attribute C has value c in a tuple. 
![[multi_to_mono.png]]
\[only the first part of this image is useful for us\]

### Quantitative attributes
What happens if there are quantitative attributes?
![[table_example_2.png]]
Simple, we use [[2022-11-11 -  Preprocessing and dissimilarities#Discretization|discretization]], with the techniques that we've used so far. 
In this way we have the same dataset as in the previous example. 

-----
## THE END!
