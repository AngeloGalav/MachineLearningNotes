# Evaluation of a classifier

## Model Selection
When we evaluate a model, we must consider a number of properties, and accuracy isn't necessarily the most important. There might be other types of errors or properties which do not influence accuracy, but are still important to consider.

i.e. if I have a model that is used to classify oil spills, which are a very rare occurrence, if our model always says that there aren't any oil spills (even if they are), our model accuracy will still be 99%. 

## Training set 
There's still the problem of _hyperparameter_ tuning. 
As [[#Overfitting|we've seen]], in supervised learning, the training set performance is __overoptimistic__.

- Evaluate how much the theory fits the data
- Evaluate the cost generated by prediction errors

- Empirically (and intuitively) the more training data we use the best performance we should expect 
- Statistically, we should expect a larger covering of the situation that can occur when classifying new data 
- We must consider the effect of random changes 
- The evaluation is independent from the algorithm used to generate the mode

### The meaning of _test error_.
Let us suppone that the test set is a good representation, on the average, of the entire dataset $X$. 

The relationship between the training set and $X$ will be subject to probabilistic variability.
The evaluation of a classifier can be done either at different levels 
- __general__ - the whole performance of the classifier 
- __local__ - the _local_ performance of a component of the model, i.e. a node of a decision tree

If test set error is X, we should expect a run-time error $x \pm ???$, meaning we should expect X and some variability. 

## Confidence interval in error estimation - Bernoulli process
Forecasting each element of the test set is like one experiment of a __Bernoulli process__.
A Bernoulli process is a repetition of binary experiments, where all the experiments are supposed to have the same probability of success (and failure).
So, in ML, a Bernoulli process looks something like this: 
- good prediction => success (S) 
- bad prediction => error (E)
the events are the same as N independent binary random events of the same type.
We define $f = E/N$ the __empirical frequency of error__ ($f$ or $e$).  
But we are not interested in the empirical frequency, but in the __true frequency of error__. 

## Empirical frequency and true frequency
Deviations of the empirical frequency from the true frequency are due to noise. Usually, noise is assumed to have a normal distribution around the true probability (for $N \ge 30$)

We choose the _confidence level_, i.e. the ==probability that the true frequency of success is below the pessimistic frequency that we will compute==.  The confidence level can be computed as:
![[confidence_level_formula.png]]
- we assign the confidence level (which allows us to compute $Z$)
- we measure the empirical frequency ($f$ in the formula)
- $N$ is the number of experiments (in practice, it is the number of elements in the test set)

We can visualize what the confidence level looks like:
($\pm \ Z_{\alpha}$ are the values such that the tails of the Gaussian have area $\alpha/2$ each). 
![[confidence_level_visualization.png]]
According to the Gaussian, if I wanto to be sure that my estimate is true, I need to go towards the tail ends of the graph. 
The pessimistic evaluation is close to the empirical frequency. 

With a little algebra we can compute this formula:
![[pessimistic_error_formula.png]]
Where, if we substitute the $\pm$ with $+$ we'll get the __pessimistic error__ $e_{max}$. 
So, if I assign $\alpha$, which is the probability that my pessimistic evaluation is wrong (i.e. 95% of confindence of being true) then:
- I measure the $f$, the empirical frequency. 
- $N$ is known
- $Z$ depends on the desired confidence level $α$ (according to the table seen in [[confidence_level_visualization.png|this image]])
	- it is the abscissa delimiting the area $1  - α$ for a normal distribution 

Then _I can compute_ the maximum error (__pessimistic error__) $e_{max}$ according to the previous formula.

Here are some ==important observations== on the formula:
- We can observe that as $Z$ increases, the amount added to $e$ to obtain $e_{max}$ increases.  
- A bigger $N$ means a _bigger sample of data_ for the __test__, so:
	- if I have a big $N$, it means that the training data is less (since test and training data are extracted from a single dataset). 
	- So the quantities that I add to compute $e_{max}$ are smaller, so ideally, _if I could use an infinite amount of test data_, the _test error_ (the uncertainty) would be _0_. 

We can see this in the following figure:
![[confidence_interval.png]]
The _purple_ one is empirical error frequency, while the _blue_ line is the maximum error frequency. 
We can see that if we have an higher number of test elements, the _uncertainty_ is reduced, meaning that empirical error is very close to the maximum error. 

## Statistical pruning with error estimation
What can we do with the statistical concepts that we've just introduced? 
We could, for example, apply the _C4.5 strategy_, and use these concepts to decide if we have to prune a tree or not. 
- Consider a subtree near the leaves
- Compute its maximum error $e_l$ as a weighted sum of the maximum error of the leaves.
- Compute the maximum error $e_r$ of the root of the subtree transformed into a leaf

Consider this subtree:
![[dt_pruning.png]]
We have to leaves $n_{121}$ and $n_{122}$. 
If we prune prune this nodes, $n12$ becomes a leaf, which _will contain $n121 + n122$ elements_. 
So, if:
$$(N_{121} + N_{122})* e_{max_{12}} \lt N_{121} * e_{max_{121}} + N_{122} * e_{max_{122}})$$
Then we can _prune_. 
This is because if we are in the described situation, we have a bigger number of example 
($N$) in the node, so the ==uncertainty ($e_{max}$) is smaller==.  
In practice, this means that when I am in the nodes at the top, the uncertainty is smaller (since I have a big number of elements), but when I come to the bottom, I have less elements so the uncertainty increases. 

Essentially, we there are tools that can tell us if we should prune a tree and how to do it. So let's consider a decision tree, and let's focus on the leaves.

What does it mean in practice?
That the bigger number of the tree is small. So, it maybe the case that if I prune part of the tree, the pessimistic error frequency is smaller. 

Obviously, this doesn't work very well with a small dataset.  

## Accuracy of a classifier
The _error frequency_ is the simplest indicator of the quality of a classifier. It is the sum of errors on any class divided by the number of tested records. 
From now on, for simplicity, we will use the __empirical error frequencies__ ($f$).  Remember that in real cases the _maximum error frequencies_ ($e_{max}$) should be used instead. 

## The hyperparameters 
Every ML algorithm has them, and they are parameters that influence its behavior. 
We need to do several tests/loops to find the best set of parameters. 
This is not so trivial, since each test can take a long time. 

# Testing strategies
The optimization process (of finding the best hyperparameters) makes use of the test set. 
Since I use the test set for both testing and optimizing, and it can take too much time to do, we have to use alternative strategies.
- __Holdout__:  if I need to do a model validation, I split the data into _training set_, _validation set_ and _test set_. 
	- (validation is the process of testing if the model is overfit). 
- __Cross validation__: it is the ==most effective solution==. 
![[cross_validaiton_example.png]]

### Cross-validation
The _training set_ is randomly _partitioned_ into $k$ subsets (if necessary, use stratified partitioning, meaning that each the proportion of classes is equal). 
- I do $k$ iterations, using one of the subsets for _test_ and the others for _training_. 
	- In this way, each record is used $k - 1$ times for _training_ and once for _testing_
- Optimal use of the supervised data.
- Typical value: $k = 10$
![[real_cross_validation_example.png]]

In the end I will make the <u>final testing with the remaining data</u>. 

Here are the tradeoff of this method: 
- The higher is the K, the less data I have for testing (and the _more iterations_ I have to perform). 

But it has a lot of advantages:
- The estimate of the performance is averaged on k runs => _more reliablity_ 
- All the examples are used once for testing 
- The final model is obtained using all the examples => _best use of the examples_

So, what can I do is doing ==cross validation to find the best== <u>hyperparameters</u>. And the test-set is considered fresh data, so it will be more reliable. 

#### Leave one out 
This is an extreme case of cross validation, in which $k = N$ (so no partitioning). It can be uses 

##### In short 
Our objectives are:
- Find the _best hyperparameters_ setting
- Give a _reliable estimation_ of the _run-time performance_, i.e. in regards to the expected ability to classify new data (overestimation of the expected performance should by avoided).

# Binary predictions
A _binary prediction_ is, for example, a case in which we have to predict if a certain attribute is true or false. In this cases, a bad prediction results in either a false positive or a false negative.
Let's consider a table like this one:
![[binary_prediction_table.png]]
TP stands for True Positive, and FN stands for false negative, etc...
This matrix/table is then extended and called a __confusion matrix__, with a frequency (i.e. a number of FPs) associated to each predicted class.   

We can compute the accuracy as $$
\dfrac{TP + TN}{N_{tests}} 
$$
### Other performance indicators
Is the accuracy the only performance indicator for a classifier? No, we have also:
- Velocity 
- Robustness w.r.t. noise i.e. training data with bad class label.
- Scalability 
- Interpretability: i.e. a decision tree has good interpretability since we see a clear path of the decision since we have a sequence of clear tests, and we can come to a conclusion.

Another dimension of indicators for classifiers is related to the _influence/impact of errors_. A classification can have different consequences depending on the class that it predicts. 
- i.e. when forecasting an illness a false positive can be less dangerous than a false negative.

For this reason, quite often an unbalanced dataset is provided, meaning that one class is present in the vast majority of cases. This way, our classifier will be able to predict a class in favor of another more reliably.  

\[During an optimization process, we can a optimize w.r.t. some of this measures (named quality measures).\]

#### A summary of measures 
Here are also some aspects that we can measure:
- _Precision_ = $TP/(TP + FP)$, it is the rate of true positives. 
- _Recall_ = $TP/(TP + FN)$ , it is the complementary of the precision. i.e. the number of positives that I can catch (a.k.a. Sensitivity). 
- _Specificity_ = $TN/(TN  + FP)$ the rate of the negatives that I can catch. 
- __F1 score__ (or F-measure) is given by the __harmonic mean__ of precision and recall:
$$
F1\_score = 2\dfrac{prec \cdot rec}{prec + rec}
$$
It is a value between 0 and 1. It increases when precision and recall are __balanced__.
It is usually interesting when we want to find hte best compromise between precision and recall. 
![[f1_score_visualization.png]]
#### Multi-dimensional case
![[multi-dimensional_case.png]]
In the multidimensional case, the definition of accuracy, precision and recall is ease (as seen in the slide). 
We can also compute some _global definitions_, which combines the values of the single classes. These combinations can be seen in the following paragraph:

### Combination strategies
\[this slide/part has been removed in the latest version of the classification slides, so I dont think it is too important.\]
![[combination_strategies.png]]

In short:
- Accuracy is important, but if we have a situation in which different types of error have different impacts, we should use precision or recall. 

## Beyond accuracy
Is it likely to obtain a _correct prediction by chance_?
- i.e. if we have a disease affecting 2% of patients, a stupid classifier always saying 'no' will  be right 98% of times (98% precision!)
i.e.:
![[confusion_matrix_beyond.png]]
![[beyond_continuation.png]]
![[beyond_3.png]]
$k(C)$ is called $k$-_score_.

## $κ$ statistic
The $k$ statistic evaluates the concordance between two classifications, in our case between the _predicted_ and _the true one_.
- Probability of concordance $P(c) = \dfrac{TP_a +TP_b + TP_c}{N}$
- Probability of random concordance $P(r) = \dfrac{TP_a*P_a +TP_b*P_b + TP_c*P_c}{N^2}$
- $κ$ is the ratio between the concordance exceeding the random component and the maximum surplus possible
$$
k = \dfrac{Pr(c) - Pr(r)}{1- Pr(r)}
$$
with $-1 \le k \le 1$.
Some rules of thumb:
![[K_rule_thumbs.png]]

## Cost of errors
Our decisions are driven by predictions. Bad predictions imply a cost.
So we can develop a _cost matrix_.
![[cost_matrix.png]]
In this case, false positives cost 5 times as the negatives. The correct classification does not imply any cost. 

### Cost sensitive learning
We have to alternative if we want to modify the learning process based on a cost.
- Alternative 1: _alterate the proportion of classes in the supervised data_, duplicating the examples for which the classification error is higher.
	- In this way, the classifier will became _more able to classify_ the classes for which the _classification error cost is higher_. 
	- This solution is useful also when the classes are imbalanced, that is the frequencies of the class labels in $X$ are not equal. 

- Alternative 2: some learning schemes allow to _add weights_ to _the instances_.
	- e.g. the `DecisionTreeClassifier` of Scikit-Learn has the hyperparameter class_weight: it allows to define a dictionary, with one key per distinct class, specifying the relative weight to be assigned to each class, the optimisation of the fitting will be adjusted accordingly.

### Predicting probabilities of classes
Many classifiers produce, rather than a _class label_ (__crisp prediction__), a _tuple of probabilities_, one for each possible class, (__probabilistic__, or soft prediction)

The adequacy of one output or the other depends on the application domain
- when an immediate decision is required the crisp output is necessary 
- when the classification is part of a process including several evaluation/action steps, the probabilistic output can be more appropriated

##### Crisp2Probability and viceversa
Crisp values sometimes hide probabilities. 
- when a leaf of a decision tree has non–zero counts for the minority classes a less–than–one probability can be assigned on the basis of on the fraction of the elements belonging to the majority class.
	- We can derive the probability. 

Probabilities can be converted to a crisp value with different techniques, depending on the number of classes (binary or multiclass) 
- binary – set a threshold for the positive class 
- multiclass – output the class with the maximum probability

#### Liftchart
A _liftchart_ is used to evaluate in various scenarios, depending on the application.
Consider a dataset with 1000 positives and apply a probabilistic classification scheme.
Sort all the classified elements for decreasing probability of positive class.
Make a bi-dimensional chart with axes $x$ = sample size, $y$ = number of positives in sample.
Only the rank is important, not the specific probability.

------
No one will read this, but this lesson was a real pain. It was very difficult to understand the meaning of his words. 