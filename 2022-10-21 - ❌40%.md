
(**Lost the first half an hour**)

## Two flavour for classification
- __Crisp__: the classifier assigns to each individual one label.
- __Probabilistic__: the classifier assigns a probability for each of the possible labels.

We will now consider the crisp classification.

## Classification with Decision Trees
A decision tree is a __run-time classifier__ structured as a decision tree, with a tree-shaped set of tests. 
The decision tree has _inner nodes_ (where we make a test) and _leaf nodes_.

#### Nodes
In these node, we make a test, such as:
```
if test on attribute d of element x then 
	execute node’ 
else execute node”
```

Leaf nodes contains a prediction: ex. `predict class of element x as c''`.
Example of a decision tree:

![[decision_tree_ex.png]]

This is a decision procedure that you can use to classify a new individual. 

The classification model can be represented as
The parameters of this decision model can be seen as 

## Model generation
Given a set $X$ of elements for which the class is known, grow a decision tree as follows:
- if all the elements belong to class $c$ or $X$ is small generate a leaf node with label $c$.
- otherwise 
	- choose a test based on a single attribute with two or more outcomes.
	- make this test the root of a tree with one branch for each of the outcomes of the test.
	- partition $X$ into subsets corresponding to the outcomes and apply recursively the procedures to the subsets.

Essentially: if we can stop, fine, otherwise we must partition the dataset. In the beginning of the tree, I have an entire dataset. According to the outcome of the inner node I split the dataset into two parts.

#### Problems to solve
1. which attribute should we test?
2. which kind of tests?
[..]

### Pairplot
A pairplot is a two-dimensional representation of a dataset. For each pair of possible attributes, a pair of scatter plots is shown. 

A diagonal is a representation on a single attribute using a single line (like an istogram). 

## Supervised learning goals

Our goal is to design an algorithm to find interesting patterns, in order to forecast the values of a n attribute given  the values of other attributes.
In our case, we must find patterns to __guess the class__ given the other values.

How much can we evaluate if a pattern is interesting?
There are several methods, one of them is based on information theory, which is primarily used in telecommunication, and is based in the concept of _entropy_.

### Entropy
Given a variable with 4 possible values and a given probability distribution $P(A) = 0.25$, 
$P(B) = 0.25$, PpCq “ 0.25, PpDq “ 0.25 an observation of the data stream could return BAACBADCDADDDA . .

I could decide to use a 2-bit encoding: 
$$
A = 00, B = 01, C = 10, D = 11
$$
Converting the transmission will be very easy (it will be $0100001001001110110011111100$).

When the probability distribution are uneven, the problem becomes quite complex.
Let's consider:
$$
P(A) = 0.5, P(B) = 0.25, P(C) = 0.125, P(D) = 0.125
$$

We could use the same encoding as the first example, but we can do better.
is there a coding requiring only 1.75 bit per symbol, on the average? YES!
$A = 0, B = 10, C = 110, D = 111$

We can improve it tho!
What if there are only three symbols with equal probability? $P(A) = 1/3, P(B) = 1/3, P(C) = 1/3$
Again, the two–bit coding shown above is still possible. 
But is there a coding requiring less than 1.6 bit per symbol, on the average? YES!
$A = 0, B = 10, C = 11$ or any permutation of the assignment

We can generalize what has been just said using these considerations:
Given a source $X$ with $V$ possible values, with probability distribution.
$$
P(v_1) = p_1, P(v_2) = p_2, \dots, P(v_v) = p_v  
$$
We will consider as information source the information of classes of each individual, so the different values will be the different labels of the classes. 

What happens if we have only 1 symbol?
Then, some probabilities will be 0 and one will be 1, and the H(x) will be 0. That means that if we have 0 variability, we have 0 entropy. 

In general:
- ==__High entropy__ means high uncertainty==, and that the probabilities are mostly similar. So the histogram would be flat. 
- __Low entropy__ means low uncertainty. 
- A higher number of allowed symbols increases the entropy.
![[entropy_histogram.png]]
In a binary source with symbol probabilities p and (1-p) when p is 0 or 1 the entropy goes to 0.

###### Entropy after a threshold–based split
Splitting the dataset in two parts according to a threshold on a numeric attribute the entropy changes, and becomes the weighted sum of the entropies of the two parts. 
The weights are the relative sizes of the two parts.

Let $d \in D$ be a real-valued attribute, let $t$ be a value of hte domain of $d$, let $c$ be the class attribute. 

We define the entropy of $c$ with respect to $d$ with threshold $t$ as:
$$
H(c|d:t) = H(c|d<t)*P(d<t) + H(c|d \ge t) * P(d \ge t)
$$

##### Information gain for binary split
It is the reduction of the entropy of a target class obtained with a split of the dataset based on a threshold for a given attribute.
We define $IG(c|d:t) = H(c) - H(c|d:t)$, it is the information gain provided when we know if, for an individual, $d$ exceeds the threshold $t$ in order to forecast the class value. 

We define $IG(c|d) = max_t \ IG(c|d:t)$.

If we plot the information gain changing the threshold, we'll se that there is a maximum. 

Ex. 
![[IG_example.png]]

The attribute that gives us the most information is the Gender. 
The attribute "LastDigitSSN" is random noise, and is negligible.

### Decision Tree generation

Which attribute should we test?
- test the attribute which guarantees the maximum IG for the class attribute in the current data set $X$ 
- partition $X$ according to the test outcomes
- recursion on the partitioned data

The supervised dataset will be split in two parts, randomly:
- One will be used as the __training set__, used to learn the model.
- The other will be used as the __test set__, used to evaluate the learned model on fresh data.

The proportion of the split is decided by the experimenter, and the parts have similar characteristics. Some of the proportions are 80-20, 67-33 and 50-50.


#### One stump decision

![[decision_tree_result_example.png]]


### Recursion step 

When there's more than one class 