[persi i primi 20 minuti]

## Neural networks

Arrange many perceptron–like elements in a hierarchical structure [..]

In animals, a neuron is a signal processor with a threshold. 
If they were linear, we would expect to have  an output correlated linearly with the input (+ a threshold/weight).

We would have a something looking like a sigmoid function (or an $arctan$).
![[functions_nn.png]]

If we want to represent learning, we would need to adjust the weights so that we get the desired output. 

The signals transmitted are modeled as real numbers: one signal - one number. 
The threshold of the biological system is modeled as a _mathematic function_. If the function is continuos and differentiable, calculations become easier. 

A perceptron is linear, so the results is non satisfactory. This is because in a linear system, we have that $f (x_1 + x_2) = f (x_1) + f (x_2)$ , so if $x_2$ presents some noise, it is completely transferred to the output. 

[..]

In linear perceptron we have added an input called bias. 

[..]

The algorithm for training neural networks is similar to something like this:
```
set all weights to random values
while termination condition is not satisfied do
	for each training instance x do
		feed the network with x and compute the output nn(x)
		compute the weight corrections for nn(x) − xOut
		propagate back the weight corrections
```

In each iteration, we would have a different starting point. 

Each training loop is called an _epoch_. Sometime an epoch can require minutes or hours, so we should terminate the loop in this case. 

We should also evaluate the change of the weights: if it is small, then we stop the iteration. 

[..]


If we have a non-convex error function, finding the local minimum becomes difficult. 
Computing the gradient is a matter of derivatives.

The error is represented as a function of the _weights_. By computing the derivatives in respect to the weights, we know in which direction to move.  

Since the derivatives are computed in respect to the sigmoid itself, computation becomes easy. 

The weight is changed subtracting ==the partial derivative== multiplied by a _learning rate_ constant. The learning rate influences the convergence speed and can be adjusted as a
__tradeoff between speed and precision__.

The algorithm to train neural networks could be rewritten in this way:
```
set all weights to random values
	while termination condition is not satisfied do
		for each training instance x do
			1 – feed the network with x and compute the output nn(x)
			2 – compute error prediction at output layer nn(x) − xOut
			3 – compute derivatives and weight corrections for output layer
			4 – compute derivatives and weight corrections for hidden layer
Steps 1 and 2 are forward, steps 3 and 4 are backward.
```

There are several variants for learning modes in machine learning. 

- Stochastic 
	- each forward propagation is immediately followed by a weight update (as in the algorithm of previous slide)
	- introduces some noise in the gradient descent process, since the gradient is computed from a single data point
	- reduces the chance of getting stuck in a local minimum
	- good for online learning
This can introduce noise, and thus make the learning process 
- Batch – many propagations occur before updating the weights,
accumulating errors over the samples within a batch
– generally yelds faster and stable descent towards the local
minimum, since the update is performed in the direction of
the average error

## Deep learning
Deep learning deals with deep networks, which are neural networks with a large number of hidden layers.
 
The idea of neural networks is very old (80s), but at the time computational power was not enough to compute deep networks, but also there were numerical porblems (like stability etc..). 

With the advent of GPUs, which are very complex parrallel computing units, we could finally deal with millions of parameters. 

Deep networks need more parameters, so also they need large dataset. 

## K-nearest neighbors classifier. 

It is the basis of clustering. 

## From binary classifier to multiclass classification. 