## Lift chart
A lift chart is what we obtain from lisiting a [..] We extract the most promising values from the dataset, and we determine how many positive values we can find. 

The __lift chart__ essentially tell us how able is our model to correctly classify objects as positive. 
We have two lines:
- The straight line plots the number of positives obtained with a random choice of a sample of test data.
- The curve plots the number of positives obtained drawing a fraction of test data with decreasing probability. 
![[lift_chat_example.png]]
The larger the area between the two lines, the better the classification model (The more the curvature is high on the left, the better is our classifier). 

The lift chart is especially useful when we have to work in batches. 

### ROC
Keep in mind: Soft classifier = probabilistic classifier. 

Let's consider that we have a lot of data, and it has some noise. Some of this noise alters our data, and we have to distinguish the data to extract knowledge from it. 
We could impose a threshold to the values, but we would have some FP and FN.

What we could do is imposing multiple thresholds, using __threshold steps__. Varying the threshold the behavior of the classifier changes, by changing the ratio of TP and FP.

So, the soft classifier can be converted into a crisp one by choosing thresholds, in a ROC curve the quality of the classifier is summarized by the __Area Under Curve__ (AUC) (the bigger the better). 

Here's how to choose the thresholds:
```
sort the test elements by decreasing positive probability 
set the threshold to the highest probability, set TP and FP to zero 
repeat 
	- update the number of TP and FP with probability from the threshold to 1 
	- draw a point in the curve 
	- move to next top probability of positive en
end repeat
```
Here's an example of a perfect selection:
![[perfect_roc.png]]

Here's an example of a random selection:
![[random_roc.png]]

# Classification - III

We now have more than one class/label. 

### Naive Bayes Classifier
It is Naive, and based on the Bayes theorem:
$$
Pr(d_1 = v_1, d_2 = v_2 | c = c_x)= Pr(d_1 = v_1 | c = c_x) \cdot Pr(d_2 = v_2 | c = c_x)
$$
We assume that each attribute is __independent__ from each other, and we consider the contribution of all the attributes. 

Here's an example.
Given this data:
![[example_data.png]]

We will have:
![[example_text.png]]
Each likelihood is calculated by using the probability of each possible event condition.

### Bayes theorem
This theorem states that, given a hypothesis $H$ and an evidence $E$ that bears on that hypothesis:
$$
Pr(H|E) = \dfrac{Pr(E|H) \ Pr(H)}{Pr(E)}
$$
In our case, the hypothesis is the class, say $c$, the evidence is the tuple of values of the element to be classified. 

We can split the evidence into pieces, one per attribute, and, if the attributes are independent inside each class, we can state that:
$$
Pr(c|E) = \dfrac{Pr(E_1|c) \ Pr(E_2|c) \ Pr(E_3|c)  \ Pr(c) }{Pr(E)}
$$
The Naive Bayes method is called _naive_ since the assumption of independence between attributes is quite simplistic Nevertheless ==it works quite well in many cases==.

##### Drawbacks
What if value v of attribute d never appears in the elements of class $c$? In this case we have that $Pr(d = v | c) = 0$ . This makes the probability of the class for that evidence drop to zero, and we would need to overcast it, which we dont want. 

We need an alternative solution. 

### Laplace smoothing
Let’s start ignoring the details of the dataset, we consider only the value domains, and we know that for a given attribute $d$ there are $V_d$ distinct values. 

Then a simple guess for the frequency of each distinct value of $d$ in each class is $1/V_d$. 

We can smooth the computation of the posterior probabilities of values inside a class balancing it with the prior probability.

The parameters are:
- $\alpha$ , the _smoothing_ parameter, typical value is 1.
- $af_{d=vi,c}$  , the absolute frequency of value vi in attribute d over class $c$.
- $V_d$ , the number of distinct values in attribute $d$ over the dataset.
- $af_c$ : absolute frequency of class c in the dataset.

The final smoothed frequency is:
$$
sf_{d=v_i, c} = \dfrac{af_{d=v_i, c} + \alpha}{af_c + \alpha V_d}
$$
The bigger is $\alpha$, the more importance we give to the prior probabilities. 

The missing values do not affect the model, and so it is not necessary to discard an instance with missing values. I.e., for decision trees, we cannot deal with missing values. 

Normally, we simpy discard that attratibute with the missing values, as if it was not present. The descriptive statistics are based on Known, non-null values. 

Depending if a value is present in the test set or in the train set, we have two ways to handle them:
- Test instance: The calculation of the likelyhood simply omits this attribute. The likelihood will be higher for all the classes, but this is compensated by the normalization. 
- Train instance: The record is simply not included in the frequency counts for that attribute. The descriptive statistics are based on the number of values that occur, rather than on the number of instances.

[slides 15 and 18]

While these solutions have a excellent values in many cases, we would have a dramatic degradation if the simplistic conditions are not met. 
- Violation of independence – for instance, if an attribute is simply a copy of another (or a linear transformation), the weight of that particular feature is enforced (something like squaring the probability)
- Violation of gaussian distribution – use the standard probability estimation for the appropriate distribution, if known, or use estimation procedures, such as Kernel Density Estimation

## Linear classification with perceptron
![[perceptron.png]]
A perceptron is also called an artificial neuron. In practice, it's a linear combination of weighted inputs. 

For a dataset with numeric attributes, we need to find (or learn) an _hyperplane_ (a straight line) such that all positives lay on one side and all the negatives on the other. 

The hyperplane is described by a set of weights $w_0, ..., w_D$ in a linear equation on the data attributes $x_0, \dots, x_D$. 
The fictitious attribute $x_0 = 1$ is added to allow a hyperplane that does not pass through the origin. There are either none or infinite such hyperplanes.
![[hyperplane_equation.png]]
[..]

Each change of weights _moves the hyperplane towards the misclassified instance_, consider the equation after the weight change for a positive instance x which was classified as negative:
$$
(x_0 + w_0)*x_0, ..., (x_D + w_D)*x_D
$$The result is increase by a positive amount: 
$$
x_0^2, \dots, x_D^2
$$

Therefore, the result will be less negative or, possibly, even positive. It is analogous for a negative instance which was classified as positive.

The corrections are incremental and can interfere with previous updates.
==The algorithm converges if the dataset is __linearly separable__==, otherwise it does not terminate. For practical applicability it is necessary to set an upper bound to the iterations.

## Support vector machine
It's a mathematical machine that tries to solve the problem of non-separability. 

A linear perceptron is linearly binary (Naive-Bayes can deal with any number of values, so as we know it is not binary.) 

How to overcome the linear separability? We could simply give up on the linearity, by using a different shape outside of the hyperplane. i.e.  ![[Screenshot_20221102_135147.png]]

This method would have some drawbacks:
- The method would become soon intractable for any reasonable number of variables. 
	- i.e.,  with 10 variables and limiting to factors with maximum order 5 we would need something like 2000 coefficient 

- The method would be extremely prone to overfitting, if the number of parameters approaches the number of examples

(Overfitting is when we are too much detailed. The more parameters we have discovered, the more the risk of overfitting.)

Computational learning theory New efficient separability of non–linear functions that use kernel functions Optimization rather than greedy search Statistical learning The search of a prediction function is modeled as a function estimation problem

When we deal with hyperplanes, we have an infinite amount of solution:
![[hyperplane_inf.png]]
In that set, the best line is this one ($B_1$):
![[hyperplane_best.png]]
The margin is defined as the distance between the lines that touch these different examples. $B_1$ is the line with the biggest margin. 
The bigger is the margin, the safer is the behaviour in respect of the new data that is similar to the older one. In this respect, it is essential to find the separation with the highest margin. 

The best separating hyperplane 
What can we do to find the __best__ separating hyperplane?

[ha fatto anche la slide 33, 34]
